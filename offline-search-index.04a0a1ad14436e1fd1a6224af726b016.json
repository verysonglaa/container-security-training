[{"body":"The command line tool First it is time to become familiar with the command line utility. Using Docker consists of passing at least one command. docker --help shows the available options:\ndocker --help Usage: docker COMMAND A self-sufficient runtime for containers Options: --config string Location of client config files (default \"/home/user/.docker\") -D, --debug Enable debug mode --help Print usage -H, --host list Daemon socket(s) to connect to -l, --log-level string Set the logging level (\"debug\"|\"info\"|\"warn\"|\"error\"|\"fatal\") (default \"info\") --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default \"/home/user/.docker/ca.pem\") --tlscert string Path to TLS certificate file (default \"/home/user/.docker/cert.pem\") --tlskey string Path to TLS key file (default \"/home/user/.docker/key.pem\") --tlsverify Use TLS and verify the remote -v, --version Print version information and quit Management Commands: config Manage Docker configs container Manage containers image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker volume Manage volumes Commands: attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes Run 'docker COMMAND --help' for more information on a command. To view the switches available to a specific command, type:\ndocker \u003ccommand\u003e --help To view system-wide information about Docker, use:\ndocker info Hello world (with Docker images) Docker containers are run from Docker images. By default, they pull these images from Docker Hub, a Docker registry managed by Docker Inc, the company behind the Docker project. Anybody can build and host their Docker images on Docker Hub, so for many applications and Linux distributions you‚Äôll find Docker images that are hosted on Docker Hub.\nTo check whether you can access and download images from Docker Hub, type:\ndocker run hello-world The output, which should include the following, indicates that Docker appears to be working correctly:\nHello from Docker. This message shows that your installation appears to be working correctly. ... Your first container üòÉ With this command, we just ran our first container on our computers. It ran a simple process that printed a message to standard out, the container itself is not very useful though.\n","categories":"","description":"","excerpt":"The command line tool First it is time to become familiar with the ‚Ä¶","ref":"/docs/container-basics/01-basics/","tags":"","title":"Docker Basics"},{"body":"Docker images You can search for images available on Docker Hub by clicking the Explore link or by typing mariadb into the search field: https://hub.docker.com/search/?q=mariadb\u0026type=image You will get a list of results and the first hit will probably be the official image: https://hub.docker.com/_/mariadb This page contains instructions on how to pull the image. Let‚Äôs pull a certain version of mariadb:\ndocker pull mariadb:11.5 Security Best Practices When using images from Docker Hub or other sources, always follow these practices:\nVerify the Image Source: Use official images or trusted sources. Vulnerability Scanning: Scan images with docker scan or a similar tool. Check SHA256 Digest: Verify your image‚Äôs digest to confirm it hasn‚Äôt been altered. Is it an official image ? Official Images are a good starting point, please read here why.\nWhat is installed in the image?\nRead the Dockerfile that was used to build the image Check the base image Check the vulnerabilities of this image. Does it affect your application? Check the dependencies of the image. Compare your images Digest to the sha256 value shown on dockerhub. After an image has been downloaded, you may then run a container using the downloaded image with the sub-command run. If an image has not been downloaded when Docker is executed with the sub-command run, the Docker client will first download the image, then run a container using it:\ndocker run hello-world:linux Note Here we use the linux tag of the hello-world image instead of using latest again. Avoid using the latest tag in production settings, as it can lead to unexpected behavior if the image updates. Always specify a version. To see the images that have been downloaded to your computer type:\ndocker images The output should look similar to the following:\nREPOSITORY TAG IMAGE ID CREATED SIZE mariadb 11.5 58730544b81b 2 weeks ago 397MB hello-world latest 1815c82652c0 2 months ago 1.84kB The hello world container you ran in the previous lab is an example of a container that runs and exits, after emitting a test message. Containers, however, can be much more useful than that, and they can be interactive. After all, they are similar to virtual machines, but they are more resource-friendly.\nFor example, let‚Äôs run a container using the downloaded image of MariaDB. The combination of the -i and -t switches gives you interactive shell access to the container:\ndocker run -it mariadb:11.5 An error has popped up!\n2022-08-09 08:19:21+00:00 [Note] [Entrypoint]: Entrypoint script for MariaDB Server 1:10.8.3+maria~jammy started. 2022-08-09 08:19:21+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql' 2022-08-09 08:19:21+00:00 [Note] [Entrypoint]: Entrypoint script for MariaDB Server 1:10.8.3+maria~jammy started. 2022-08-09 08:19:21+00:00 [ERROR] [Entrypoint]: Database is uninitialized and password option is not specified You need to specify one of MARIADB_ROOT_PASSWORD, MARIADB_ALLOW_EMPTY_ROOT_PASSWORD and MARIADB_RANDOM_ROOT_PASSWORD ü§î Why do I get an error? Is this a bug in the image? Everything is fine, to run this image there is some configuration needed. Read the following excerpt carefully.\nerror: database is uninitialized and password option is not specified You need to specify one of MARIADB_ROOT_PASSWORD, MARIADB_ALLOW_EMPTY_ROOT_PASSWORD and MARIADB_RANDOM_ROOT_PASSWORD We will add the configuration later.\nü§î What's an image? Think of an image as a blueprint of what will be in a container when it runs.\nAn image is a collection of files + some metadata (or in technical terms: those files form the root filesystem of a container) Images are made of layers, conceptually stacked on top of each other Each layer can add, change or remove files Images can share layers to optimize disk usage, transfer times, and memory use You build these images using Dockerfiles (in later labs) Images are immutable, you cannot change them after the creation ü§î What's the difference between a container and an image? When you run an image, it becomes a container.\nAn image is a read-only filesystem A container is an encapsulated set of processes running in a read-write copy of that filesystem To optimize container boot time, copy-on-write is used instead of regular copy docker run starts a container from a given image ","categories":"","description":"","excerpt":"Docker images You can search for images available on Docker Hub by ‚Ä¶","ref":"/docs/container-basics/01-basics/01/_images/","tags":"","title":"Images"},{"body":"Docker architecture Docker is a client-server application The Docker daemon (or ‚ÄúEngine‚Äù) Receives and processes incoming Docker API requests The Docker client Talks to the Docker daemon via the Docker API We‚Äôll use mostly the CLI embedded within the Docker binary The Docker Hub Registry Is a collection of public images The Docker daemon talks to it via the registry API The Docker installation will install the Docker daemon and client on your workstation.\nSetup introduction This training depends on an installation of Docker. If you are attending an official training you are given a pre-installed environment, you can skip this step.\nIf not, follow the instructions on the subsequent pages to complete the setup on your platform of choice.\n","categories":"","description":"","excerpt":"Docker architecture Docker is a client-server application The Docker ‚Ä¶","ref":"/setup/","tags":"","title":"Setup"},{"body":"Why was there an error in the previous lab?\nThe MariaDB server cannot run without a proper configuration. Docker can pass configuration variables into the setup process via environment variables. Environment variables are passed with the -e parameter, as in:\ndocker run -it -e MARIADB_ROOT_PASSWORD=my-secret-pw mariadb After running the command, you will see output similar to this:\nInitializing database PLEASE REMEMBER TO SET A PASSWORD FOR THE MariaDB root USER! To do so, start the server, then issue the following commands: '/usr/bin/mysqladmin' -u root password 'new-password' '/usr/bin/mysqladmin' -u root -h \u003chost\u003e password 'new-password' Alternatively, you can run: '/usr/bin/mysql_secure_installation' Notice that we used the arguments -it (interactive terminal). You may have also found that MariaDB does not respond to the usual CTRL-C. To exit, Docker provides an escape sequence to detach from a container while leaving it running: press CTRL-p, then CTRL-q. In some web shells, these shortcuts may not work, so close the terminal and reopen it to continue.\nTo verify the container is running, use:\ndocker ps Output should look like this:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7cb31f821233 mariadb \"docker-entrypoint...\" 5 minutes ago Up 5 minutes 3306/tcp upbeat_blackwell Accessing the container To reconnect to the container, use:\ndocker exec -it \u003ccontainer\u003e bash Here, \u003ccontainer\u003e can be the CONTAINER ID (typically the first two characters suffice) or the NAMES from docker ps. In the example above, this could be 7cb31f821233 or upbeat_blackwell.\nNote The docker exec command requires either the ID or name of the container, plus the command to execute, such as bash for interactive use. Executing the command should display:\nroot@7cb31f821233:/#\nNote Each time you connect to a container, you will be the user defined in the Dockerfile. Now that we‚Äôre connected, let‚Äôs check if MariaDB is working:\nmariadb -uroot -pmy-secret-pw If successful, the MariaDB command line should appear:\nWelcome to the MariaDB monitor. Commands end with ; or \\g. ... MariaDB [(none)]\u003e Type exit; to leave the MariaDB client, then type exit again to leave the container.\nDetached containers Running a Docker container in ‚Äúdetached‚Äù mode (background) can be done with -d:\ndocker run -it -e MARIADB_ROOT_PASSWORD=my-secret-pw -d mariadb Now, instead of seeing container logs, only the container ID is displayed. Verify by listing running containers:\ndocker ps Stop any container as needed:\ndocker stop \u003ccontainer\u003e Or, to remove a container, use:\ndocker rm \u003ccontainer\u003e For a complete list, including stopped containers:\ndocker ps --all Mounting a volume in a container ü§î What happens to my data when I remove the container? It‚Äôs deleted. Containers don‚Äôt store data permanently without a persistence layer, so let‚Äôs address that.\nThe MariaDB container is a good example for using a persistent volume. We‚Äôll create a Docker-managed volume for persistent MariaDB data:\ndocker volume create volume-mariadb docker run --name mariadb-container-with-external-volume -v volume-mariadb:/var/lib/mysql -e MARIADB_ROOT_PASSWORD=my-secret-pw -d mariadb Inspect your volume with:\ndocker volume inspect volume-mariadb To add a new user to MariaDB, connect and run:\ndocker exec -it mariadb-container-with-external-volume mariadb -uroot -pmy-secret-pw Inside MariaDB:\nuse mysql; CREATE USER 'peter'@'%' IDENTIFIED BY 'venkman'; GRANT SELECT ON mysql.user TO 'peter'@'%'; Now quit MariaDB and the container:\nexit By using volumes, we have persisted the data in our database!\n","categories":"","description":"","excerpt":"Why was there an error in the previous lab?\nThe MariaDB server cannot ‚Ä¶","ref":"/docs/container-basics/01-basics/01/_env-vars/","tags":"","title":"Environment variables"},{"body":"\nContainer Basics and Security","categories":"","description":"","excerpt":"\nContainer Basics and Security","ref":"/docs/","tags":"","title":"Labs"},{"body":"Frontend Let‚Äôs create a frontend to demonstrate port-forwarding and container connections. We will run a simple Python web server that displays all users in our MariaDB.\nFirst, get the IP of the currently running MariaDB container. By default, all containers are started in the bridge network, where no DNS service is available, so we can‚Äôt use container names. As a workaround, we use the IP of the container:\nexport ip=$(docker inspect mariadb-container-with-external-volume -f '{{ range.NetworkSettings.Networks }}{{ .IPAddress }}{{ end }}') docker inspect \u003ccontainer\u003e shows details about a running container in JSON format (run it yourself to explore). We filtered the JSON output to get only the container‚Äôs IP address.\nAlternatively, we could filter the output with grep, as in docker inspect mariadb-container | grep IPAddress, but our solution is more concise üòä.\nNext, we‚Äôll start the frontend container. Fortunately, an image is available online. If you‚Äôre interested, you can check the source code here :\ndocker run -d --name frontend -e username=peter -e password=venkman -e servername=$ip grafgabriel/container-lab-frontend How do we access it? Try connecting to the server using the container‚Äôs Docker-assigned IP address:\ndocker inspect frontend -f '{{ range.NetworkSettings.Networks }}{{ .IPAddress }}{{ end }}' This will show only the IP of the container as output:\n172.17.0.4 Since we don‚Äôt have a browser in the web shell, use curl http://172.17.0.4:5000 to view the page in your terminal. On a local installation, you could simply open http://172.17.0.4:5000 in your browser.\ncurl http://172.17.0.4:5000 ","categories":"","description":"","excerpt":"Frontend Let‚Äôs create a frontend to demonstrate port-forwarding and ‚Ä¶","ref":"/docs/container-basics/01-basics/01/_frontend/","tags":"","title":"Frontend"},{"body":"Installation for Windows Please follow the instructions on Docker‚Äôs official documentation to install Docker CE for Windows.\nWhen asked to use Windows container, choose NOT to.\nNote You don‚Äôt have to register for a Docker Cloud account. Shell recommendation for Windows We highly recommend to use the Bash emulation Git Bash from Git for Windows to do the exercises in this training.\nProxy configuration for Windows If your organization has a proxy in place you have to set the proxy environment variables in order to be able to do docker pull or docker push.\nGit Bash:\nexport HTTP_PROXY=\"http://\u003cusername\u003e:\u003cpassword\u003e@\u003cproxy\u003e:\u003cport\u003e\" export HTTPS_PROXY=\"http://\u003cusername\u003e:\u003cpassword\u003e@\u003cproxy\u003e:\u003cport\u003e\" Note If you have special characters in your password, you have to encode them according to Percent-encoding reserved characters . See also setting the proxy environment variables on Windows for alternative instructions on setting proxy environment variables.\nNext steps When you‚Äôre ready to go, head on over to the labs and begin with the training!\n","categories":"","description":"","excerpt":"Installation for Windows Please follow the instructions on Docker‚Äôs ‚Ä¶","ref":"/setup/01/","tags":"","title":"Installation for Windows"},{"body":" Please download the slides from moodle Read more\n","categories":"","description":"","excerpt":" Please download the slides from moodle Read more\n","ref":"/slides/","tags":"","title":"Slides"},{"body":"Dockerfile Docker builds container images by reading instructions from a ‚ÄòDockerfile‚Äô or, more broadly, a ‚ÄòContainerfile.‚Äô\" The basic docs on how Dockerfiles work can be found at https://docs.docker.com/engine/reference/builder/ .\nWrite your first Dockerfile Let us have a general look at how to build a container image. For that, create a new directory with an empty Dockerfile in there.\nmkdir myfirstimage cd myfirstimage touch Dockerfile Open Dockerfile in your preferred text editor and add the following instructions:\nFROM ubuntu RUN apt-get update \u0026\u0026 \\ apt-get install -y figlet \u0026\u0026 \\ apt-get clean FROM indicates the base image for our build Each RUN line will be executed by Docker during the build Our RUN commands must be non-interactive (no input can be provided to Docker during the build) Check https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/ for further best practices on how to write Dockerfiles. Build the image Use the following command to build your image:\ndocker build -t myfirstimage . -t indicates the tag to apply to the image . indicates the location of the build context (which we will talk more about later, but is basically the directory where our Dockerfile is located) Please note that the tag can be omitted in most Docker commands and instructions. In that case, the tag defaults to latest. Besides being the default tag there‚Äôs nothing special about latest. Despite its name, it does not necessarily identify the latest version of an image.\nDepending on the build system it can point to the last image pushed, to the last image built from some branch, or to some old image. It can even not exist at all.\nBecause of this, you must never use the latest tag in production, always use a specific image version.\nAlso see: https://medium.com/@mccode/the-misunderstood-docker-tag-latest-af3babfd6375 What happens when we build the image The output of the Docker build looks similar to this:\n[+] Building 13.3s (6/6) FINISHED docker:default =\u003e [internal] load build definition from Dockerfile 0.1s =\u003e =\u003e transferring dockerfile: 127B 0.0s =\u003e [internal] load metadata for docker.io/library/ubuntu:latest 1.4s =\u003e [internal] load .dockerignore 0.1s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [1/2] FROM docker.io/library/ubuntu:latest@sha256:99c35190e22d294cdace2783ac55effc69d32896daaa265f0bbedbcde4fbe3e5 3.1s =\u003e =\u003e resolve docker.io/library/ubuntu:latest@sha256:99c35190e22d294cdace2783ac55effc69d32896daaa265f0bbedbcde4fbe3e5 0.1s =\u003e =\u003e sha256:99c35190e22d294cdace2783ac55effc69d32896daaa265f0bbedbcde4fbe3e5 6.69kB / 6.69kB 0.0s =\u003e =\u003e sha256:5d070ad5f7fe63623cbb99b4fc0fd997f5591303d4b03ccce50f403957d0ddc4 424B / 424B 0.0s =\u003e =\u003e sha256:59ab366372d56772eb54e426183435e6b0642152cb449ec7ab52473af8ca6e3f 2.30kB / 2.30kB 0.0s =\u003e =\u003e sha256:ff65ddf9395be21bfe1f320b7705e539ee44c1053034f801b1a3cbbf2d0f4056 29.75MB / 29.75MB 0.9s =\u003e =\u003e extracting sha256:ff65ddf9395be21bfe1f320b7705e539ee44c1053034f801b1a3cbbf2d0f4056 1.7s =\u003e [2/2] RUN apt-get update \u0026\u0026 apt-get install -y figlet \u0026\u0026 apt-get clean 8.1s =\u003e exporting to image 0.3s =\u003e =\u003e exporting layers 0.2s =\u003e =\u003e writing image sha256:9739703b2866e9100738cefc2f5c7cb0b9cd2b005e5770829a7608ca55bdcfb5 0.0s =\u003e =\u003e naming to docker.io/library/myfirstimage First: sending the build context to Docker transferring context: 2B ... The build context is the . directory given to docker build It is sent (as an archive) to the Docker daemon by the Docker client This allows you to use a remote machine to build using local files Be careful (or patient) if that directory is big and your connection is slow Second: building step execution ... [1/2] FROM docker.io/library/ubuntu:latest =\u003e =\u003e resolve docker.io/library/ubuntu:latest =\u003e =\u003e sha256:99c35190e22d294cdace2783ac55effc69d32896daaa265f0bbedbcde4fbe3e5 6.69kB / 6.69kB [...] =\u003e =\u003e extracting sha256:ff65ddf9395be21bfe1f320b7705e539ee44c1053034f801b1a3cbbf2d0f4056 [2/2] RUN apt-get update \u0026\u0026 apt-get install -y figlet \u0026\u0026 apt-get clean 8.1s =\u003e exporting to image 0.3s =\u003e =\u003e exporting layers 0.2s =\u003e =\u003e writing image sha256:9739703b2866e9100738cefc2f5c7cb0b9cd2b005e5770829a7608ca55bdcfb5 0.0s =\u003e =\u003e naming to docker.io/library/myfirstimage This output shows the stages involved in building an image from a Dockerfile. Here‚Äôs a breakdown of each step:\n[internal] load build definition from Dockerfile (0.1s): Docker reads the Dockerfile and loads its contents.\n[internal] load metadata for docker.io/library/ubuntu:latest (1.4s): Docker fetches metadata for the ubuntu:latest image from Docker Hub to check if it‚Äôs up-to-date.\n[internal] load .dockerignore (0.1s): Docker loads .dockerignore to exclude specific files from the build context.\n[1/2] FROM docker.io/library/ubuntu:latest (3.1s): Docker starts downloading the ubuntu:latest image with a specific hash sha256:99c35190....\nsha256 entries represent different image layers: For example, ff65ddf... is a 29.75 MB layer, which Docker downloads and extracts. [2/2] RUN apt-get update \u0026\u0026 apt-get install -y figlet \u0026\u0026 apt-get clean (8.1s): Docker executes the RUN command to:\nUpdate the package list (apt-get update), Install the figlet package, and Clean up cached package files (apt-get clean) to reduce image size. exporting to image (0.3s): Docker finalizes the build by:\nExporting the layers, Writing the image with identifier sha256:9739703..., and Naming the image myfirstimage in the Docker library. The entire build process took 13.3 seconds, with most time spent downloading layers and installing packages.\nThe caching system If you run the same build again, it will be instantaneous. Why?\nAfter each build step, Docker takes a snapshot Before executing a step, Docker checks if it has already built the same sequence Docker uses the exact strings defined in your Dockerfile: RUN apt-get install figlet cowsay is different from RUN apt-get install cowsay figlet RUN apt-get update is not re-executed when the mirrors are updated All steps after a modified step are re-executed since the filesystem it‚Äôs based on may have changed You can force a rebuild with docker build ‚Äìno-cache ‚Ä¶\nRun it Now run your image\ndocker run -ti myfirstimage You‚Äôll find yourself inside a Bash shell in the container, execute\nfiglet hello and you will see the following output:\nroot@00f0766080ed:/# figlet hello _ _ _ | |__ ___| | | ___ | '_ \\ / _ \\ | |/ _ \\ | | | | __/ | | (_) | |_| |_|\\___|_|_|\\___/ root@00f0766080ed:/# exit the container by executing:\nexit The CMD instruction in Dockerfile With the CMD instruction in the Dockerfile, we can define the command that is executed when a container is started.\nü§î Can you find out which CMD instruction the ubuntu image uses? You did find yourself in a shell, so the instruction must either be /usr/bin/bash or /usr/bin/sh.\nTo find out the CMD instruction of the official Ubuntu image, run:\ndocker inspect ubuntu | grep -A2 CMD You will see the command defined for the ubuntu image. You can also check the Docker Hub page for Ubuntu to see details on what‚Äôs included in the image.\nModify the previously created Dockerfile as follows:\nFROM ubuntu RUN apt-get update \u0026\u0026 \\ apt-get install -y figlet \u0026\u0026 \\ apt-get clean CMD [\"figlet\", \"hello\"] Build the image with:\ndocker build -t myfirstimagecmd . And run it:\ndocker run -ti myfirstimagecmd It directly executes the defined command and prints out\n_ _ _ | |__ ___| | | ___ | '_ \\ / _ \\ | |/ _ \\ | | | | __/ | | (_) | |_| |_|\\___|_|_|\\___/ Check out https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact for more information.\nFrontend app image build Now we are familiar with the image building process we have a more detailed look at our frontend image. You can find the source code here .\nWe see that the developer has become quite lazy and has not updated the python to the latest version, he did not even care to create sensible tags. So let us do it ourselves using the tag v1.0\nCheck out the repository\ncd /home/project git clone https://github.com/songlaa/container-lab-fronted cd container-lab-fronted You should have the necessary knowledge now to update and rebuild the image locally. Delete the currently running container and start a new one with updated python.\nüò≥ I'm lost, show me the solution First of all, we need to check for the latest python base image. You could do this in dockerhub:\ngrep FROM Dockerfile We see that currently, we use version 3.9 of python, a look at https://hub.docker.com/_/python shows us that that the most recent version, at the time of writing is 3.12.\nReplace the from line with this new value\nFROM python:3.12-slim And then we build it using tag v1.0\ndocker build -t container-lab-frontend:v1.0 . docker images Finally, we kill the currently running container and start our new one, hopefully we still have $ip saved in our shell:\ndocker stop frontend docker rm frontend docker run -d --name frontend -e username=peter -e password=venkman -e servername=$ip container-lab-frontend:v1.0 ü§î What did we update by rebuilding the image? We did not only update python to a recent version but also the modules in python! Generally, you should build \u0026 deploy often to avoid configuration drift and keep your software up to date! A common solution to update your dependencies is https://docs.renovatebot.com/ ","categories":"","description":"","excerpt":"Dockerfile Docker builds container images by reading instructions from ‚Ä¶","ref":"/docs/container-basics/01-basics/01/_dockerfile/","tags":"","title":"Dockerfile"},{"body":"Installation for Mac Please follow the instructions on Docker‚Äôs official documentation to install Docker CE for Mac.\nNote You don‚Äôt have to register for a Docker Cloud account. Proxy configuration for Mac If your organization has a proxy in place you have to set the proxy environment variables in order to be able to do docker pull or docker push.\nexport http_proxy=\"http://\u003cusername\u003e:\u003cpassword\u003e@\u003cproxy\u003e:\u003cport\u003e\" export https_proxy=\"http://\u003cusername\u003e:\u003cpassword\u003e@\u003cproxy\u003e:\u003cport\u003e\" Note If you have special characters in your password, you have to encode them according to Percent-encoding reserved characters . See also setting the proxy environment variables on Mac for alternative instructions on setting proxy environment variables.\nNext steps When you‚Äôre ready to go, head on over to the labs and begin with the training!\n","categories":"","description":"","excerpt":"Installation for Mac Please follow the instructions on Docker‚Äôs ‚Ä¶","ref":"/setup/02/","tags":"","title":"Installation for Mac"},{"body":"Installation for Linux Please follow the instructions for your appropriate distribution to install Docker. The recommended way of installing is using the repository, except if you already know you‚Äôre going to remove the package again soon.\nUbuntu Fedora Debian CentOS Unrelated to what distribution you use, also have a look at the Post-installation steps for Linux . Please note however that these are optional steps and some are quite advanced, so going with the default might be the most appropriate way to go.\nProxy configuration for Linux If your organization has a proxy in place you have to set the proxy environment variables in order to be able to do docker pull or docker push.\nexport http_proxy=\"http://\u003cusername\u003e:\u003cpassword\u003e@\u003cproxy\u003e:\u003cport\u003e\" export https_proxy=\"http://\u003cusername\u003e:\u003cpassword\u003e@\u003cproxy\u003e:\u003cport\u003e\" Note If you have special characters in your password, you have to encode them according to Percent-encoding reserved characters . Next steps When you‚Äôre ready to go, head on over to the labs and begin with the training!\n","categories":"","description":"","excerpt":"Installation for Linux Please follow the instructions for your ‚Ä¶","ref":"/setup/03/","tags":"","title":"Installation for Linux"},{"body":"Often you‚Äôre going to use some kind of libraries, tools or dependencies during the build phase of your application that are not necessary during the runtime of the container. To improve security and efficiency we only include whats absolutely necessary in the image. So we often remove these dependencies in the build phase after the application itself has been built.\nIn this lab you‚Äôre going to learn how to use multistage builds and what they are good for.\nPurpose If the application is not available as a prebuilt artifact, in many cases, the application itself gets built directly during the docker build process docker build -t ...\nJava Spring Boot Gradle build Example The complete example can be found at https://github.com/appuio/example-spring-boot-helloworld .\nFROM registry.access.redhat.com/ubi9/openjdk-17 LABEL org.opencontainers.image.authors=\"midcicd@puzzle.ch\" \\ io.k8s.description=\"APPUiO Example Spring Boot App\" \\ io.k8s.display-name=\"APPUiO Spring Boot App\" \\ io.openshift.expose-services=\"8080:http\" \\ io.openshift.tags=\"springboot\" EXPOSE 8080 9000 RUN mkdir -p /tmp/src/ ADD . /tmp/src/ RUN cd /tmp/src \u0026\u0026 sh gradlew build --no-daemon RUN ln -s /tmp/src/build/libs/springboots2idemo*.jar /deployments/springboots2idemo.jar During the docker build the actual application source code is added to the context and built using the gradlew build command. Gradle in this case is only used during the build phase since it produces a jar that is then executed with java -jar ... at execution time.\nBuild phase dependencies:\nJava Gradle Runtime phase dependencies:\nJava Multi-stage builds With multistage builds you now have the possibility to actually split these two phases, so that you can pass the built artifact from phase one into the runtime phase, without the need to install build time dependencies in the resulting docker image. This means that the image will be smaller and consist of less unneeded dependencies.\nRead more about Docker multi-stage builds at https://docs.docker.com/develop/develop-images/multistage-build/ Create a multi-stage build Turn the docker build from the first example (Java Spring boot https://github.com/appuio/example-spring-boot-helloworld ) into a docker multistage build. As a second image you can use registry.access.redhat.com/ubi9/openjdk-17-runtime. Try to find the solution before looking at it.\nPlease create two separate images to see the actual size difference as well.\nShow me the solution We start by cloning the repository and building the original image:\ncd /home/project git clone https://github.com/appuio/example-spring-boot-helloworld.git cd example-spring-boot-helloworld docker build -t example-spring-boot-helloworld:v0.1 . Now let us build the next version using an optimized Dockerfile, change the content of Dockerfile to the text below:\nFROM registry.access.redhat.com/ubi9/openjdk-17 AS build LABEL org.opencontainers.image.authors=\"noreply@acend.ch\" \\ io.k8s.description=\"acend example spring boot app\" \\ io.k8s.display-name=\"acend spring boot app\" \\ io.openshift.expose-services=\"8080:http\" \\ io.openshift.tags=\"springboot\" RUN mkdir -p /tmp/src/ ADD . /tmp/src/ RUN cd /tmp/src \u0026\u0026 sh gradlew build --no-daemon FROM registry.access.redhat.com/ubi9/openjdk-17-runtime EXPOSE 8080 9000 COPY --from=build /tmp/src/build/libs/springboots2idemo*.jar /deployments/springboots2idemo.jar Now build a new version of the image and compare the size:\ndocker build -t example-spring-boot-helloworld:v0.2 . docker images ","categories":"","description":"","excerpt":"Often you‚Äôre going to use some kind of libraries, tools or ‚Ä¶","ref":"/docs/container-basics/01-basics/01/_dockerfile_ms/","tags":"","title":"MultiStage Build"},{"body":"Try Docker without installation The page https://training.play-with-docker.com offers additional tutorials which also come with an interactive shell. The disadvantage is that you have to create an account, but if you don‚Äôt want to install Docker locally, this is a great way to do the exercises in this training using a browser-based Docker shell.\nTo do this lab with Play with Docker:\nGo to https://labs.play-with-docker.com Click on Login Enter your Docker login or register first Click ADD NEW INSTANCE and you are ready to do this training Next steps When you‚Äôre ready to go, head on over to the labs and begin with the training!\n","categories":"","description":"","excerpt":"Try Docker without installation The page ‚Ä¶","ref":"/setup/04/","tags":"","title":"Try Docker without installation"},{"body":"A closer look at the docker command and the runtime We‚Äôve learned that the term ‚ÄúDocker‚Äù is used somewhat imprecisely. It refers to various components such as the CLI (Command Line Interface), the Docker Engine, the OCI image format, and the Container runtime. Let‚Äôs take a closer look at what‚Äôs happening when we use the command:\ndocker run --rm -d --name sleep-container alpine sleep 900 We will see the meaning of -rm and the other arguments later. For now, we only need to know that we started a container that sleeps for 900 seconds on our host. First let us get the process id of the sleep process we just started:\ndocker inspect --format '{{.State.Pid}}' sleep-container Let us see the process running. In the webshell we have the docker backend running in another container, let us first change into that:\nkubectl exec $(kubectl get pod -l \"app.kubernetes.io/name\"=webshell -o name) -it -c dind -- sh Don‚Äôt worry the command will make sense after the Kubernetes Security training.\nLet us see the process running on the host now, we don‚Äôt have the necessary software installed in our lab so we are installing it now as well:\nPID=$(pgrep sleep) apk add procps ps -u root -U root --forest -f | grep -B1 $PID We see something like this\nroot 50221 1 0 17:00 ? 00:00:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 724930591e3fbf44f9cacb60285c0420464c41f5a6366e2b4443c2b53e6cd251 -address /run/containerd/containerd.sock root 53658 50221 0 17:00 ? 00:00:00 \\_ sleep 900 Indeed we see that we don‚Äôt use docker as a container runtime but containerd at a higher level and runc at a lower level. The parent of each of these containerd-shim-runc-v2 processes is PID 1 on the system.\nThe shim becomes the parent process of the containerized application. It is responsible for tasks such as reaping zombie processes, handling container process I/O (standard input, output, error), and ensuring proper container cleanup upon exit. As a result, containerd can upgrade and restart without affecting running containers.\nSecondly, we see that in the end a container is just a process running on the host. If nothing else is configured it runs as root! Let us see the different isolation techniques being used (again we need to install some software for that):\napk add util-linux lsns -p $PID Which shows use the different (and newly created) namespaces being used for this container:\nNS TYPE NPROCS PID USER COMMAND 4026531834 time 463 1 root /sbin/init splash # time isolation 4026531837 user 423 1 root /sbin/init splash # uid gid isolation (root inside is not root outside) 4026534002 mnt 1 53658 root sleep 900 4026534003 uts 1 53658 root sleep 900 # hostname isolation 4026534004 ipc 1 53658 root sleep 900 4026534005 pid 1 53658 root sleep 900 4026534006 net 1 53658 root sleep 900 4026534064 cgroup 1 53658 root sleep 900 By comparision, a simple sleep command in the current shell would run in the same namespaces as the parent shell giving no isolation.\nDon‚Äôt forget to exit our Docker backend container if you work in the webshell\nexit ","categories":"","description":"","excerpt":"A closer look at the docker command and the runtime We‚Äôve learned that ‚Ä¶","ref":"/docs/container-basics/01-basics/01/_deepdive/","tags":"","title":"Under the hood"},{"body":"Security is important during the entire lifecycle of a container. We will work on various aspects of development and deployment of images as well applied the principle of least privilege to containers and the runtime. It‚Äôs important to configure container isolation, manage user privileges effectively, and follow security best principles (CIA) when orchestrating containers at scale.\n","categories":"","description":"","excerpt":"Security is important during the entire lifecycle of a container. We ‚Ä¶","ref":"/docs/container-basics/02-security/","tags":"","title":"2. Securing Containers"},{"body":"Secure Software Development Lifecycle Docker security covers the entire lifecycle of containers, including their runtime, build process, and orchestration. Key security areas include base images, Dockerfiles, container runtimes, and securing the Docker daemon. Additionally, it‚Äôs important to configure container isolation, manage user privileges effectively, and follow security best practices when orchestrating containers at scale.\nTo ensure we fully understand and manage our workload, it‚Äôs crucial to focus on our Software Development Lifecycle (SDLC). A key part of this is adopting a Secure Software Development Lifecycle (SSDLC) , which integrates security into every stage of development and deployment. One important aspect of a SSDLC is knowing exactly what components are in the software we‚Äôre building and running.\nSBOMs and vulnerabilities A Software Bill of Materials (SBOM) is a detailed list of all the components, libraries, and dependencies used in a software application. It acts like a product inventory for software, allowing developers, security teams, and stakeholders to know exactly what goes into an application. This transparency helps in identifying vulnerabilities, managing licensing risks, and ensuring compliance. With an SBOM, organizations can quickly assess the impact of security vulnerabilities or breaches, as they have a clear view of all the third-party and open-source components in their software stack.\nRecent security issues like the Log4j vulnerability and the SolarWinds breach have underscored the need to know exactly what components are being used in software to mitigate risks quickly. By incorporating SBOMs into CI/CD pipelines, developers can automate the tracking of software dependencies, detect vulnerabilities earlier, and ensure compliance with security standards, reducing the chances of introducing insecure components into production.\nWe have several tools to track the dependencies that our application/images are using. A open-source and easy-to-use one is trivy . Let us try it out with our multi-stage image we built in the previous lab, in case you did not use the tag v0.1 you can add it with docker tag example-spring-boot-helloworld:YOURTAG example-spring-boot-helloworld:v0.1:\ntrivy image --format spdx-json --output result.json example-spring-boot-helloworld:v0.1 Sometimes the trivy API gets overwhelmed with requests and reports an Error, just try again after a minute if that happens. A workaround is to use an image from another registry:\ntrivy image --db-repository public.ecr.aws/aquasecurity/trivy-db:2 --format spdx-json --output result.json example-spring-boot-helloworld:v0.1 Once done, the scanner will scan the files/image and determine which language is the application written in. Once determined, it will download the database about that specific language, get the list of libraries that are present in that language and check against which are being used in the current context. We can now examine that file and examine all libraries and packages installed in the image\njq . result.json Finally, we can check if there are currently known vulnerabilities in these dependencies:\ntrivy sbom result.json We created the SBOM explicitly to show the process, in reality, the command can be abbreviated to a simple trivy image command:\ntrivy image example-spring-boot-helloworld:v0.1 As you can see, we obtain the library name, CVE vulnerability number, severity level (HIGH, MEDIUM, LOW), vulnerability status (fixed, not fixed, or will not fix), and if fixed, the version with the fix, along with detailed information about the vulnerability.\nWith this data, we can upgrade libraries with fixes, assess the risk level of unfixed vulnerabilities, and remove unnecessary vulnerable libraries. Additionally, we have the opportunity to explore alternative libraries that are more secure.\nIn our case, we might find some vulnerable java libaries which need to be updated in the file build.gradle. If you have some experience with gradle you can try to fix it and build a new image.\nIn SSLDC scanning tools like this are mostly part of a mandatory step in a CI/CD Pipeline before uploading the image to a registry. Generally, CVE‚Äôs with a score up to a certain threshold are accepted and the rest are blocked.\n","categories":"","description":"","excerpt":"Secure Software Development Lifecycle Docker security covers the ‚Ä¶","ref":"/docs/container-basics/02-security/02/_ssdlc/","tags":"","title":"SSDLC"},{"body":"This lab focuses on understanding and securing image distribution. We‚Äôll start with a simple docker pull and build up to using Docker Content Trust (DCT).\nLet us start with a common pull\ndocker pull alpine:edge This command will pull the Alpine image tagged as edge. The corresponding image can be found here on Docker Store .\nIf no tag is specified, Docker will pull the image with the latest tag.\n$ docker pull alpine:edge edge: Pulling from library/alpine e587fa4f6e1f: Pull complete Digest: sha256:e5ab6f0941eb01c41595d35856f16215021a941e9893501d632ed4c0ee4e53a6 Status: Downloaded newer image for alpine:edge Now run a new container from the image and ping songlaa.com\ndocker run --rm -it alpine:edge ping songlaa.com Pulling by tag is easy and convenient. However, tags are mutable, and the same tag can refer to different images over time. For example, you can add updates to an image and push the updated image using the same tag as a previous version of the image. This scenario where a single tag points to multiple versions of an image can lead to bugs and vulnerabilities in your production environments.\nThis is why pulling by digest is such a powerful operation. Thanks to the content-addressable storage model used by Docker images, we can target pulls to specific image contents by pulling by digest. In this step you‚Äôll see how to pull by digest.\nPull the Alpine image with the sha256:b7233dafbed64e3738630b69382a8b231726aa1014ccaabc1947c5308a8910a7 digest.\ndocker pull alpine@sha256:b7233dafbed64e3738630b69382a8b231726aa1014ccaabc1947c5308a8910a7 It‚Äôs not easy to find the digest of a particular image tag. This is because it is computed from the hash of the image contents and stored in the image manifest. The image manifest is then stored in the Registry. This is why we needed a docker pull by tag to find digests previously. It would also be desirable to have additional security guarantees such as image freshness.\nEnter Docker Content Trust: a system currently in the Docker Engine that verifies the publisher of images without sacrificing usability. Docker Content Trust implements The Update Framework (TUF), an NSF-funded research project succeeding Thandy of the Tor project. TUF uses a key hierarchy to ensure recoverable key compromise and robust freshness guarantees.\nUnder the hood, Docker Content Trust handles name resolution from IMAGE tags to IMAGE digests by signing its own metadata ‚Äì when Content Trust is enabled, docker will verify the signatures and expiration dates in the metadata before rewriting a pull by tag command to a pull by digest.\nIn this step you will enable Docker Content Trust and pull signed and unsigned images.\nEnable Docker Content Trust by setting the DOCKER_CONTENT_TRUST environment variable.\nexport DOCKER_CONTENT_TRUST=1 All Docker commands remain the same. Docker Content Trust will work silently in the background. Pull the alpine signed image.\ndocker pull alpine That works because the image is trusted.\nNow try to pull an untrusted image\ndocker pull grafgabriel/alpine It will fail with a similar error (also please never use this image, it is really old)\nError: remote trust data does not exist for docker.io/grafgabriel/alpine: notary.docker.io does not have trust data for docker.io/grafgabriel/alpine If you have Content Trust enabled you can only download trusted images. If you try to push images with content trust enabled Docker will ask you to create a key to sign your images and it will then sign your image before uploading it.\nDocker Content Trust is powered by Notary , an open-source TUF -client and server that can operate over arbitrary trusted collections of data. Notary has its own CLI with robust features such as the ability to rotate keys and remove trust data.\nDisable Content Trust again as we will need to download unsigned images in our lab:\nexport DOCKER_CONTENT_TRUST=0 For more information about Docker Content Trust, see the documentation .\nOfficial images All images in Docker Hub under the library organization (currently viewable at: https://hub.docker.com/explore/ ) are deemed ‚ÄúOfficial Images.‚Äù These images undergo a rigorous, open-source review process to ensure they follow best practices. These best practices include signing, being lean, and having clearly written Dockerfiles. For these reasons, it is strongly recommended that you use official images whenever possible.\nOfficial images can be pulled with just their name and tag. You do not have to precede the image name with library/ or any other repository name.\n","categories":"","description":"","excerpt":"This lab focuses on understanding and securing image distribution. ‚Ä¶","ref":"/docs/container-basics/02-security/02/_trust/","tags":"","title":"Docker Trust"},{"body":"Before exploring different options to minimize container privileges, it‚Äôs important to address a fundamental yet frequently overlooked practice: keeping your software up to date. Regular updates are crucial for protecting against known container escape vulnerabilities, such as Leaky Vessels , which often allow attackers to gain root access to the host. This means both the host system and Docker itself must be consistently updated, including the host kernel and Docker Engine.\nSince containers share the host‚Äôs kernel, a vulnerable kernel exposes all containers to risk. For instance, the Dirty COW kernel privilege escalation exploit , even if run inside a highly isolated container, would still lead to root access on a vulnerable host.\nUser Management in Docker We learnt that if nothing else is configured a user within a container runs as root. Configuring the container to use an unprivileged user is the best way to prevent privilege escalation attacks. This can be accomplished in three different ways.\nRun as a different user First, during runtime using the -u option of docker run. Please check the differences between the two commands:\ndocker run alpine id docker run -u guest alpine id Note that the users we are running as must exist in the /etc/passwd of the Docker container. Otherwise, the command will fail as it fails to resolve the username to a user entry in the /etc/passwd file. As an alternative, you can run it using an arbitrary uid. In all cases, the user must have the necessary rights to execute the binaries or read the files needed in the container. For alpine this works because most binaries are set to read/execute for everyone (755).\nAdd USER to Dockerfile A second way is to set it in the image. Simply add a user and the USER instruction to the Dockerfile. Here is an example:\nFROM alpine RUN groupadd -r myuser \u0026\u0026 useradd -r -g myuser myuser # \u003cHERE DO WHAT YOU HAVE TO DO AS A ROOT USER LIKE INSTALLING PACKAGES ETC.\u003e USER myuser Let us apply those two ways to our frontend container as well. By checking the process on your local host we see that we started this container, despite our current knowledge, with root privileges:\ndocker top frontend Which shows an output like similiar to this:\nUID PID PPID C STIME TTY TIME CMD root 19086 19067 1 22:45 ? 00:00:00 /usr/local/bin/python3.9 /usr/local/bin/flask run --host=0.0.0.0 --port=5000 Let us stop and start the container with a user who does not exist on the host:\ndocker stop frontend docker rm frontend docker run -u 1001 -d --name frontend -e username=peter -e password=venkman -e servername=$ip container-lab-frontend:v1.0 docker top frontend Ok, this is fine but even better would be to have it as a default already in the Dockerfile. Please change the Dockerfile of the frontend application to use a new user and build it with a tag of v2.0. Try to do it on your own before checking the solution.\nI'm lost, show me the solution First, make sure you are in the right directory:\ncd /home/project/container-lab-fronted Second, change your Dockerfile to match the content below:\n# Use an official Python runtime as a parent image FROM python:3.12-slim # Set the working directory WORKDIR /app # Install required packages RUN pip install flask mysql-connector-python # Create a non-root user and group RUN groupadd -r appuser \u0026\u0026 useradd -r -g appuser appuser # For the installation we were root, now switch to the non-root user USER appuser # Copy the current directory contents into the container at /app COPY . /app # Set environment variable for Flask ENV FLASK_APP=app.py # Expose port 5000 for the Flask app EXPOSE 5000 # Define the default command to run the app with Flask CMD [\"flask\", \"run\", \"--host=0.0.0.0\", \"--port=5000\"] Now build it using the tag v2.0 make sure you are in the frontend directory:\ndocker build -t container-lab-frontend:v2.0 . Now we stop the currently running container and start our new one:\ndocker stop frontend docker rm frontend export ip=$(docker inspect mariadb-container-with-external-volume -f '{{ range.NetworkSettings.Networks }}{{ .IPAddress }}{{ end }}') docker run --name frontend -d -e username=peter -e password=venkman -e servername=$ip container-lab-frontend:v2.0 docker top frontend Configure the Docker Daemon to user USER namespaces Both ways to change to user are fine. But what, if you need to run an image that requires root privileges inside the container?\nThis is where the third option comes into play. It makes use of the Linux USER namespace to re-map the root user within the container to a less-privileged user in the host machine.\nIn this way, the container will be running as root, but that root is mapped to a user that has no privileges on the host. User namespaces are not enabled by default and require to modify the start parameters for the docker daemon.\nMore on that topic in the official documentation .\nOther container runtimes like podman automatically enable user namespaces, there is an excellent article on that topic here , if you want to read more.\nWhy to avoid running as root We learned that although we are root in a container there are restrictions like cgroups, namespaces and capabilities in place so why care?\nThere are multiple ways to gain elevated privileges in docker, by having multiple security layers in depth in place we make it harder for an attacker in case of an exploit or a misconfiguration is in place.\n","categories":"","description":"","excerpt":"Before exploring different options to minimize container privileges, ‚Ä¶","ref":"/docs/container-basics/02-security/02/_root/","tags":"","title":"Avoid Root"},{"body":"Understanding container capabilities Capabilities in Linux are fine-grained controls that are part of the POSIX permissions system. These capabilities allow you to limit or extend the privileges of a process. Container capabilities are a set of predefined permissions that control what operations a container can perform on the host system. By default, containers run with a wide set of capabilities, but in many cases, they do not require all of them, so giving them only the permissions they need makes them safer to use.\nSome of the most commonly used capabilities in Docker include:\nCAP_NET_BIND_SERVICE: Allows binding to ports below 1024 (e.g., running a web server on port 80). CAP_SYS_ADMIN: Provides broad system administration privileges (often considered too powerful for most use cases). CAP_CHOWN: Allows changing file ownership. CAP_DAC_OVERRIDE: Allows bypassing file read, write, and execute permission checks. By default, a container has a wide range of these capabilities, but you can restrict or grant specific capabilities using configuration options when starting the container.\nCheck the default capabilites of a container:\ndocker run --rm -it alpine sh -c 'apk add -U libcap; capsh --print' Important for us are the sections Current and Bounding Set, these are the capabilites a process in our container has or can acquire. You can deny processes from gaining more privileges by adding --security-opt=no-new-privileges to the docker run command.\nFurthermore, we have Current Inheritable and Blocked (IAB) Capabilities , this shows the capabilities that the process does NOT have. Each capability here is prefixed with !, indicating that the capability is disabled or not granted.\nConfiguring a Container to Use Only What It Needs In Docker, you can drop unnecessary capabilities or explicitly add only the required ones using the ‚Äìcap-drop and ‚Äìcap-add options in the command line.\nExample: Configuring a Web Server with Minimal Capabilities Consider running an Nginx web server inside a Docker container. By default, the Nginx process needs to bind to port 80, but it doesn‚Äôt need many other elevated privileges.\nLet us optimize our Frontend Application even further. We stop it and then start it with no privileges at all:\ndocker stop frontend docker rm frontend export ip=$(docker inspect mariadb-container-with-external-volume -f '{{ range.NetworkSettings.Networks }}{{ .IPAddress }}{{ end }}') docker run --name frontend -d -e username=peter -e password=venkman -e servername=$ip --cap-drop ALL --security-opt=no-new-privileges container-lab-frontend:v2.0 Let‚Äôs see if it is still running:\nfrontendIP=$(docker inspect frontend -f '{{ range.NetworkSettings.Networks }}{{ .IPAddress }}{{ end }}') curl http://$frontendIP:5000 You should still see the available users in the backend database, we just dropped all CAPs the process in the container doesn‚Äôt need to implement a least privileges strategy.\n","categories":"","description":"","excerpt":"Understanding container capabilities Capabilities in Linux are ‚Ä¶","ref":"/docs/container-basics/02-security/02/_capabilities/","tags":"","title":"Capabilities"},{"body":"So far we configured user and process permissions of the container. Another important step is to check the filesystem permissions and mount options of a container.\nA common method is to run the containers with a read-only filesystem. Let us try to write into a read-only mounted filesystem:\ndocker run --rm --read-only alpine sh -c 'echo \"whatever\" \u003e /tmp/blub' The command fails with an error:\nsh: can't create /tmp/blub: Read-only file system If you still need to write temporary files, we can do that using the --tmpfs, this will create a temporary in-memory filesystem which is gone as soon at the container is stopped.\nTry it using:\ndocker run --rm --read-only --tmpfs /tmp alpine sh -c 'echo \"whatever\" \u003e /tmp/blub' In addition, if the volume is mounted only for reading, mount them as read-only. It can be done by appending :ro to the -v. Here is an example:\ndocker run -v volume-name:/path/in/container:ro alpine We continue improving security for our frontend application by adding these options to our docker run command.\ndocker stop frontend docker rm frontend docker run --name frontend -d -e username=peter -e password=venkman -e servername=$ip --cap-drop ALL --security-opt=no-new-privileges --read-only --tmpfs /tmp container-lab-frontend:v2.0 You can check now with curl that our frontend is still running fine:\nfrontendIP=$(docker inspect frontend -f '{{ range.NetworkSettings.Networks }}{{ .IPAddress }}{{ end }}') curl http://$frontendIP:5000 ","categories":"","description":"","excerpt":"So far we configured user and process permissions of the container. ‚Ä¶","ref":"/docs/container-basics/02-security/02/_volumes/","tags":"","title":"Volume security"},{"body":"Introduction to Linux Security Modules Linux Security Modules (LSMs) provide mechanisms for implementing various security policies in Linux. They help in enforcing access controls and secure applications by restricting their capabilities and interactions with the system. The most popular LSMs are:\nSeccomp (Secure Computing Mode):\nSeccomp provides a way to filter system calls that a process can make. Defining a list of allowed or disallowed system calls can minimize the attack surface of applications by reducing the risk of exploitation through system call vulnerabilities. This allows you to create even more granular control over the system calls than you would have using capabilities.\nAppArmor (Application Armor):\nAppArmor uses profiles to confine applications and restrict their access to system resources. Each profile defines the permissions for a specific application, including which files it can access, which network operations it can perform, and more. AppArmor is generally easier to manage and configure compared to SELinux. There is a default enabled AppArmor profile for Docker usually named docker-default. You can find the template for this profile here .\nSELinux (Security-Enhanced Linux):\nSELinux provides a robust mechanism for supporting access control policies. It enforces security policies that dictate how processes interact with each other and with system resources, based on labels assigned to files, processes, and other objects. SELinux offers more granularity and flexibility than AppArmor but can be more complex to configure.\nApplying Seccomp to our frontend container Seccomp can be used to restrict the system calls available to a container, thereby limiting its potential attack surface. Here‚Äôs how you can apply a Seccomp profile to an Nginx container:\nFirst, create a Seccomp profile in JSON format. For example, create a file named frontend-seccomp.json under /home/project with the following content to restrict some potentially risky system calls:\n{ \"defaultAction\": \"SCMP_ACT_ALLOW\", \"syscalls\": [ { \"names\": [ \"accept\", \"bind\", \"connect\", \"getcwd\", \"getdents\", \"getpid\", \"recvfrom\", \"sendto\", \"socket\" ], \"action\": \"SCMP_ACT_ALLOW\" } ] } This profile allows only a subset of system calls necessary for the frontend to operate, blocking others. Adding another layer of defense in addition to our dropped capabilities.\nTo apply this profile to our frontend container, you can use the Docker command line with the --security-opt option:\ncd /home/project docker stop frontend docker rm frontend docker run --name frontend -d -e username=peter -e password=venkman -e servername=$ip \\ --cap-drop ALL --security-opt=no-new-privileges \\ --read-only --tmpfs /tmp \\ --security-opt seccomp=frontend-seccomp.json container-lab-frontend:v2.0 You can check if the Seccomp profile is applied by inspecting the container:\ndocker inspect frontend | grep seccomp And again as a final check, we test if our service is still available:\nfrontendIP=$(docker inspect frontend -f '{{ range.NetworkSettings.Networks }}{{ .IPAddress }}{{ end }}') curl http://$frontendIP:5000 AppArmor, Seccompm or SELinux can also play important roles in mitigating unpatched vulnerabilities like Leaky Vessels ","categories":"","description":"","excerpt":"Introduction to Linux Security Modules Linux Security Modules (LSMs) ‚Ä¶","ref":"/docs/container-basics/02-security/02/_lsm/","tags":"","title":"Linux Security Modules"},{"body":"Do the following commands and read the outputs carefully:\nwhoami head -1 /etc/shadow Now do this:\ndocker run -v /etc:/host alpine sh -c 'whoami;head -1 /host/shadow' Why can you suddenly read stuff that you shouldn‚Äôt be allowed to? Are containers not restricted? The answer lies in the architecture of Docker:\nBy default, the Docker Deamon runs in root mode without user namespaces enabled. This means we can mount anything from the host into our container and change it there too using the root user inside our container.\nIt gets worse, just read the excerpt below as we don‚Äôt have sudo installed in our webshell:\nuser2@localhost$ whoami user2 user2@localhost$ sudo su Sorry, user user2 may not run sudo on localhost. user2@localhost$ docker run --rm -it -v /etc/sudoers.d:/host/etc/sudoers.d alpine sh / # echo 'user2 ALL=(ALL) NOPASSWD:ALL' \u003e /host/etc/sudoers.d/user2 / # exit user2@localhost$ sudo su root@localhost$ whoami root To ensure that a user running a container doesn‚Äôt gain root access to your host, you run the container engine and the containerized process as a non-root user. This provides multiple layers of security between the service (httpd, MySQL, etc.) and the privileged resources in the operating system.\nRunning the container engine as a non-root user, is one layer of defense, while running the process in the container as a different non-root user offers yet another layer of defense.\nHere is an excerpt from the docs : Rootless mode executes the Docker daemon and containers inside a user namespace. This is very similar to userns-remap mode, except that with userns-remap mode, the daemon itself runs with root privileges, whereas in rootless mode, both the daemon and the container are running without root privileges.\nRunning rootless mode in Docker comes with a set of limitations, most notably that not all storage drivers are allowed and AppArmor is not supported.\nAnother solution would be to switch from Docker to Podman .\nWhile Docker relies on a client-server model, Podman employs a daemonless architecture. With Podman‚Äôs approach, users manage containers directly, eliminating the need for a continuous daemon process in the background.\nCompared to Docker it has stronger default security settings, features like rootless containers, user namespaces, and seccomp profiles are enabled by default. On the image below we see a comparison between Docker and Podman architecture.\nRecap We saw how to run containers and how to secure them avoiding root, dropping capabilities, mounting filesystems read-only and using Linux Security Modules such as seccomp. However, it is important to say that because of the architecture of docker, anyone who can start a container has more privileges on the host. It is still important to secure the Host Operating system and maybe to run a deamonless container technology like podname. What we did not touch are things like network security and monitoring. More on that in the Kubernetes Security lab.\n","categories":"","description":"","excerpt":"Do the following commands and read the outputs carefully:\nwhoami head ‚Ä¶","ref":"/docs/container-basics/02-security/02/_runtime_sec/","tags":"","title":"Container Runtime Security"},{"body":"A note on privileged containers The ‚Äìprivileged option in Docker is a special flag that gives the container full access to the host system. It‚Äôs much more powerful than simply assigning specific capabilities because it bypasses most of Docker‚Äôs built-in security restrictions and grants the container elevated permissions, similar to running as root on the host. Often times you will see it in tutorials for running Docker inside Docker or similar things.\nWhen a Docker container is run with the ‚Äìprivileged flag, several key things happen:\nAll Capabilities Granted Full Device Access (Containers usually have limited access to devices on the host system (like /dev, which includes devices like disks, USBs, etc.)) AppArmor and Seccomp Disabled Unrestricted Network Access It‚Äôs recommended to avoid using ‚Äìprivileged unless necessary and to use capabilities and specific device access options instead for better security and control.\nStart a privileged container We refrain from starting a privileged container on our shared infra. So please read the instructions in the /home/project/welcome file to access a dedicated VM with ssh:\ncat /home/project/welcome Copy and execute the relevant part:\nssh -i id-edcsa userx@192.168.0.1 This VM has docker already installed. Before starting a privileged container first check the mounted file systems, then just execute this command to run and access a privileged container:\ndf docker run -it --rm --privileged ubuntu /bin/bash Now try to escape this container from the inside (try to read a file like /etc/passed on the host). This time we don‚Äôt provide a solution.\n","categories":"","description":"","excerpt":"A note on privileged containers The ‚Äìprivileged option in Docker is a ‚Ä¶","ref":"/docs/container-basics/02-security/02/_privileged-mode/","tags":"","title":"Privileged Containers"},{"body":"We saw how to run containers and how to secure them avoiding root, dropping capabilities, mounting filesystems read-only, and using Linux Security Modules such as seccomp. However it is important to say that because of the architecture of docker, anyone who can start a container has more privileges on the host.\nIt is still important to secure the Host Operating system and maybe to run a deamonless container technology like podname. What we did not touch are things like network security and monitoring. More on that in the Kubernetes Security lab.\nIf you are still good on time you can play around with escaping Docker containers yourself:\n(Optional) Docker Breakout We will explore Docker breakout techniques to demonstrate potential privilege escalation risks in Docker environments. The exercises will guide you through creating Docker containers, testing breakout scenarios, and understanding the mechanisms that can be exploited to gain unauthorized access outside the container.\nEscape via Docker Socket Run a Container that binds the Docker socket i.e. with -v /var/run/docker.sock:/var/run/docker.sock. Use Docker commands within the container to explore breakout opportunities.\nHost PID Namespace Access Execute the following command:\nsleep 900 \u0026 docker run -it --rm --pid=host ubuntu /bin/bash This runs a Container with --pid=host. Explore how this enables access to host processes. Attempt to kill the sleep process.\n(Optional) Podman You can install Podman alongside docker to test it out:\nsudo apt -y install podman Try different user commands with podman. Try to start a pod. Try with buildah to build an image.\n","categories":"","description":"","excerpt":"We saw how to run containers and how to secure them avoiding root, ‚Ä¶","ref":"/docs/container-basics/02-security/02/_recap/","tags":"","title":"Recap"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":" Container Security Training Setup Labs Slides ","categories":"","description":"","excerpt":" Container Security Training Setup Labs Slides ","ref":"/","tags":"","title":"Container Security Training"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]